{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Neg=os.path.join(os.getcwd(),\"neg\")\n",
    "Pos=os.path.join(os.getcwd(),\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neg_Doc=[]\n",
    "Pos_Doc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDocs(Folder,Type_Doc):\n",
    "    for file in os.listdir(Folder):\n",
    "        File=open(os.path.join(Folder,file),'rt')\n",
    "        text=File.read()\n",
    "        File.close()\n",
    "        Type_Doc.append(text)\n",
    "    return Type_Doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neg_Doc=GetDocs(Neg,Neg_Doc)\n",
    "Pos_Doc=GetDocs(Pos,Pos_Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCleanedDoc(DocList):\n",
    "    from nltk import word_tokenize\n",
    "    from nltk import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    Cleaned_List=[]\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    for doc in DocList:\n",
    "        tokens=word_tokenize(doc)\n",
    "        word=[]\n",
    "        for token in tokens:\n",
    "            if token.isalpha() and token not in stop_words:\n",
    "                word.append(stemmer.stem(token))\n",
    "        Cleaned_List.append(word)\n",
    "    return Cleaned_List   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neg_Cleaned=GetCleanedDoc(Neg_Doc)\n",
    "Pos_Cleaned=GetCleanedDoc(Pos_Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting training , testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=1200\n",
    "testing_size=800\n",
    "split=int(training_size/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neg_train=Neg_Cleaned[:split]\n",
    "Neg_test=Neg_Cleaned[split:]\n",
    "Pos_train=Pos_Cleaned[:split]\n",
    "Pos_test=Pos_Cleaned[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_labels=np.zeros((training_size,1))\n",
    "test_labels=np.zeros((testing_size,1))\n",
    "train_labels[split:]=1\n",
    "test_labels[split:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSet(Neg,Pos):\n",
    "    Dataset=[]\n",
    "    for neg in Neg:\n",
    "        Dataset.append(neg)\n",
    "    for pos in Pos:\n",
    "        Dataset.append(pos)\n",
    "    return Dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=CreateSet(Neg_train,Pos_train)\n",
    "test_dataset=CreateSet(Neg_test,Pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Train Dataset :- train_dataset\n",
    "\n",
    "Train Label   :- train_labels\n",
    "\n",
    "Test Dataset  :- test_dataset\n",
    "\n",
    "Test Labels   :- test_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "Data={ 'train_dataset':train_dataset,\n",
    "       'train_labels':train_labels,\n",
    "       'test_dataset':test_dataset,\n",
    "       'test_labels':test_labels \n",
    "     }\n",
    "with open('Data.pickle','wb') as handle:\n",
    "     pickle.dump(Data,handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
