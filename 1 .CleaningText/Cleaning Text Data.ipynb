{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text is Messy\n",
    "\n",
    "You cannot go straight from raw text to fitting a machine learning or deep learning model.\n",
    "\n",
    "You must clean your text first, which means splitting it into words and normalizing issues such as:\n",
    "\n",
    "Upper and lower case characters.\n",
    "\n",
    "Punctuation within and around words.\n",
    "\n",
    "Numbers such as amounts and dates.\n",
    "\n",
    "Spelling mistakes and regional variations.\n",
    "\n",
    "Unicode characters\n",
    "\n",
    "and much moreâ€¦\n",
    "### Manual Tokenization\n",
    "\n",
    "Generally, we refer to the process of turning raw text into something we can model as \"tokenization\", where we are left with a list of words or \"tokens\".\n",
    "\n",
    "We can manually develop Python code to clean text, and often this is a good approach given that each text dataset must be tokenized in a unique way.\n",
    "\n",
    "For example, the snippet of code below will load a text file, split tokens by whitespace and convert each token to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '...'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lowercase\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Tokenization\n",
    "Many of the best practices for tokenizing raw text have been captured and made available in a Python library called the Natural Language Toolkit or NLTK for short.\n",
    "\n",
    "You can install this library using pip by typing the following on the command line:\n",
    "\n",
    "\n",
    "\n",
    "    pip install -U nltk\n",
    "\n",
    "After it is installed, you must also install the datasets used by the library, either via a Python script:\n",
    "\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download()\n",
    "Or via a command line:\n",
    "\n",
    "\n",
    "    python -m nltk.downloader all\n",
    "Once installed, you can use the API to tokenize text. For example, the snippet below will load and tokenize an ASCII text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = '...'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
