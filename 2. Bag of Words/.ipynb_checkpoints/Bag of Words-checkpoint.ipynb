{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer well defined fixed-length inputs and outputs.\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Specifically, vectors of numbers\n",
    "This is called feature extraction or feature encoding.\n",
    "\n",
    "A popular and simple method of feature extraction with text data is called the bag-of-words model of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words is a representation of text that describes the occurrence of words within a document.\n",
    "\n",
    "A vocabulary is chosen, where perhaps some infrequently used words are discarded. A given document of text is then represented using a vector with one position for each word in the vocabulary and a score for each known word that appears (or not) in the document.\n",
    "\n",
    "It is called a \"bag\" of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "1.A vocabulary of known words.\n",
    "\n",
    "2.A measure of the presence of known words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with scikit-learn\n",
    "The scikit-learn Python library for machine learning provides tools for encoding documents for a bag-of-words model.\n",
    "\n",
    "An instance of the encoder can be created, trained on a corpus of text documents and then used again and again to encode training, test, validation and any new data that needs to be encoded for your model.\n",
    "\n",
    "There is an encoder to score words based on their count called CountVectorizer, one for using a hash function of each word to reduce the vector length called HashingVectorizer, and a one that uses a score based on word occurrence in the document and the inverse occurrence across all documents called TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "\n",
    "\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "#vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[0.         0.78980693 0.         0.         0.         0.\n",
      "  0.         0.61335554]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dwijraj\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform([text[1]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with Keras\n",
    "The Keras Python library for deep learning also provides tools for encoding text using the bag-of words-model in the Tokenizer class.\n",
    "\n",
    "As above, the encoder must be trained on source documents and then can be used to encode training data, test data and any other data in the future. The API also has the benefit of performing basic tokenization prior to encoding the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('the', 4), ('quick', 1), ('brown', 1), ('fox', 2), ('jumped', 1), ('over', 1), ('lazy', 1), ('dog', 2)])\n",
      "3\n",
      "{'the': 1, 'fox': 2, 'dog': 3, 'quick': 4, 'brown': 5, 'jumped': 6, 'over': 7, 'lazy': 8}\n",
      "{'lazy': 1, 'brown': 1, 'over': 1, 'dog': 2, 'jumped': 1, 'fox': 2, 'quick': 1, 'the': 3}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "docs=text\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
